{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Amazon Reviews Sentiment",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sifatkhan-1915020/deeplearning-/blob/main/Amazon_Reviews_Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'amazon-reviews:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3868600%2F6713271%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241012%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241012T143356Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5c0ea9b8e1a7d79167f6ad0e95c38b1dc1ffc1be4f98848174befecd1b592005ddcdc797b9f3347fa1ab5e89a86ff7c609a1c7f5a90804616ddf294f35de949f08c99e6f1deefe0770a725a47cebdcb142e8e8577f1650d2c037e48e8f75a211518011ef4739fbecbe7a179de1e51012cc52db201ce014b99f531b4b817cae73d5ff3c698b91611c9ddc46147676212c72b79e66433aa5093235d95cb8b73a9bf27b1aa6e11857fa4b7991761bf669d23d691e5d20a358f46df890ccdb3ff7cdc9646f477bc51d188e08efcb27726fe356511bfb42ee2d8764bb35846dabf674de96ac02faed49954e875c6e75b17541194ac7b4868ef70e4a6ec9a243ea39d2'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "LsvwfKtFJozO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "bav0cPazJozS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score , f1_score, accuracy_score,confusion_matrix\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.layers import Dense , Embedding , Bidirectional , LSTM\n",
        "\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "a4GGvdX2JozT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "AhyhqyISJozT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json('Amazon reviews.json' , lines = True)"
      ],
      "metadata": {
        "id": "WrVN7dT0JozU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "vQWzCO5yJozU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['reviewerID', 'asin', 'reviewerName', 'helpful','summary', 'unixReviewTime', 'reviewTime'],axis='columns',inplace=True)"
      ],
      "metadata": {
        "id": "6-37NO7IJozU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= df.rename(columns={\"reviewText\":\"Review\",\"overall\": \"Rating\"})"
      ],
      "metadata": {
        "id": "JdAWBU3pJozU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review_len'] = [len(text.split()) for text in df.Review]"
      ],
      "metadata": {
        "id": "lf64CsauJozV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~(df['review_len'] < 20) & ~(df['review_len'] > 40)]"
      ],
      "metadata": {
        "id": "BzzjVxKBJozV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_label(df) :\n",
        "    if df['Rating'] <= 3.0 :\n",
        "        rate = 0 # for Negative\n",
        "    else :\n",
        "        rate = 1 # for Positive\n",
        "\n",
        "    return rate"
      ],
      "metadata": {
        "id": "FDDfeqjdJozV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Rating'] = df.apply(convert_label , axis = 1)"
      ],
      "metadata": {
        "id": "vrLJxC2VJozV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "cN_Nw9anJozW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "tHPMStzzJozW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_count = df['Rating'].value_counts()\n",
        "fig,axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
        "\n",
        "sns.set_theme(style='darkgrid', palette='pastel')\n",
        "color = sns.color_palette(palette='pastel')\n",
        "explode = [0.02]*len(label_count)\n",
        "\n",
        "axes[0].pie(label_count.values, labels=label_count.index, autopct='%1.1f%%', colors=color, explode=explode)\n",
        "axes[0].set_title('Percentage Label')\n",
        "\n",
        "sns.countplot(df['Rating'] , ax=axes[1])\n",
        "axes[1].set_title('Count Label')\n",
        "axes[1].set_xlabel('Label')\n",
        "axes[1].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SkbttQmuJozW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MostWordsUsed(txt , n_words) :\n",
        "    all_text = ''.join(df[txt].values)\n",
        "\n",
        "    all_text = re.sub(r'\\d+', '', all_text) # numbers\n",
        "    all_text = re.sub(r'[^\\w\\s]', '', all_text) # special characters\n",
        "\n",
        "    words = all_text.split()\n",
        "\n",
        "    # remove puncs\n",
        "    punc = list(punctuation)\n",
        "    words = [w for w in words if w not in punc]\n",
        "\n",
        "    # remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word.lower() for word in words if not word in stop_words]\n",
        "\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    top_words = word_counts.most_common(n_words)\n",
        "\n",
        "    return top_words"
      ],
      "metadata": {
        "id": "DYM_YS-fJozW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words = MostWordsUsed('Review' , 40)\n",
        "\n",
        "xaxis = [word[0] for word in top_words]\n",
        "yaxis = [word[1] for word in top_words]\n",
        "\n",
        "plt.figure(figsize=(16,5))\n",
        "plt.bar(xaxis , yaxis)\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Most Commonly Used Words', fontsize=25)\n",
        "plt.xticks(rotation=45)\n",
        "plt.subplots_adjust(bottom=0.15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oc2-Y9jGJozW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.countplot(x='review_len', data=df[(df['review_len']<=1000) & (df['review_len']>10)], palette='Blues_r')\n",
        "plt.title('Count of sentence with high number of words', fontsize=25)\n",
        "plt.yticks([])\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KllTcb8oJozX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "YZhPO39LJozX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DataPrep(text) :\n",
        "    text = re.sub(r'\\d+', '', text) # numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # special characters\n",
        "\n",
        "    # tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # remove puncs\n",
        "    punc = list(punctuation)\n",
        "    words = [word for word in tokens if word not in punc]\n",
        "\n",
        "    # remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word.lower() for word in words if not word in stop_words]\n",
        "\n",
        "    # lemmatization\n",
        "    words = [lemma.lemmatize(word) for word in words]\n",
        "\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "iuh7Ma8qJozX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_reviews'] = df['Review'].apply(DataPrep)"
      ],
      "metadata": {
        "id": "F3qjmF5EJozX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are around {int(df[\"cleaned_reviews\"].duplicated().sum())} duplicated reviews, we will remove them.')"
      ],
      "metadata": {
        "id": "EfBgzuK4JozX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(\"cleaned_reviews\", inplace=True)"
      ],
      "metadata": {
        "id": "JDF1TUAYJozX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the data"
      ],
      "metadata": {
        "id": "wicR2IIsJozX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train , x_val , y_train , y_val = train_test_split(df['cleaned_reviews'] , df['Rating'] , train_size = 0.80 , random_state = 42)"
      ],
      "metadata": {
        "id": "yvX-O3g-JozX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train) , len(x_val)"
      ],
      "metadata": {
        "id": "Xp50_VksJozY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "MV_NJGVKJozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec = TfidfVectorizer()\n",
        "vec.fit(x_train)\n",
        "print(\"No. of feature words: \",len(vec.get_feature_names()))"
      ],
      "metadata": {
        "id": "9255g0wXJozY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vec.transform(x_train).toarray()\n",
        "x_val = vec.transform(x_val).toarray()"
      ],
      "metadata": {
        "id": "mLS8wmsjJozY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape , x_val.shape"
      ],
      "metadata": {
        "id": "-HD2ofhFJozY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "dbYCSuj_JozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(random_state=42)\n",
        "lr.fit(x_train , y_train)"
      ],
      "metadata": {
        "id": "tAu8zATdJozZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc1 = lr.score(x_train , y_train)"
      ],
      "metadata": {
        "id": "7wnKLjHIJozZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_pred = lr.predict(x_val)\n",
        "\n",
        "val_acc1 = accuracy_score(y_val , lr_pred)\n",
        "\n",
        "val_precision1 = precision_score(y_val , lr_pred , average='weighted')\n",
        "val_recall1 = recall_score(y_val , lr_pred , average='weighted')\n",
        "val_f1score1 = f1_score(y_val , lr_pred , average='weighted')"
      ],
      "metadata": {
        "id": "jd4J-273JozZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The training accuracy for logistic regression : {(train_acc1*100):0.2f}%\\n\")\n",
        "print(f\"The validation accuracy for logistic regression : {(val_acc1*100):0.2f}%\\n\")\n",
        "print(f\"The precision for logistic regression : {val_precision1:0.2f}\\n\")\n",
        "print(f\"The recall for logistic regression : {val_recall1:0.2f}\\n\")\n",
        "print(f\"The f1 score for logistic regression : {val_f1score1:0.2f}\\n\")"
      ],
      "metadata": {
        "id": "rZ6Uv4CGJozZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_cm = confusion_matrix(y_val , lr_pred)\n",
        "sns.heatmap(lr_cm, annot=True,fmt='3g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "usdpQPwqJoza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Classifier"
      ],
      "metadata": {
        "id": "Z1hwYzSxJoza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(x_train , y_train)"
      ],
      "metadata": {
        "id": "AI0wr5mjJoza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc2 = rf.score(x_train , y_train)"
      ],
      "metadata": {
        "id": "gBdWWVdZJoza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_pred = rf.predict(x_val)\n",
        "\n",
        "val_acc2 = accuracy_score(y_val , lr_pred)\n",
        "\n",
        "val_precision2 = precision_score(y_val , rf_pred , average='weighted')\n",
        "val_recall2 = recall_score(y_val , rf_pred , average='weighted')\n",
        "val_f1score2 = f1_score(y_val , rf_pred , average='weighted')"
      ],
      "metadata": {
        "id": "i6PCHQkgJozb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The training accuracy for Random Forest : {(train_acc2*100):0.2f}%\\n\")\n",
        "print(f\"The validation accuracy for Random Forest : {(val_acc2*100):0.2f}%\\n\")\n",
        "print(f\"The precision for Random Forest : {val_precision2:0.2f}\\n\")\n",
        "print(f\"The recall for Random Forest : {val_recall2:0.2f}\\n\")\n",
        "print(f\"The f1 score for Random Forest : {val_f1score2:0.2f}\\n\")"
      ],
      "metadata": {
        "id": "wtuECNo2Jozb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_cm = confusion_matrix(y_val , rf_pred)\n",
        "sns.heatmap(lr_cm, annot=True,fmt='3g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BAvqIivOJozb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "z_yq6GDIJozb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train , X_val , Y_train , Y_val = train_test_split(df['cleaned_reviews'] , df['Rating'] , train_size = 0.80 , random_state = 42)"
      ],
      "metadata": {
        "id": "-8U3mdUDJozc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train) , len(X_val)"
      ],
      "metadata": {
        "id": "Y-vTc8eRJozc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [word for text in df['cleaned_reviews'] for word in text.split()]\n",
        "words_count = Counter(corpus)\n",
        "sorted_words = words_count.most_common()"
      ],
      "metadata": {
        "id": "yaHWdZdTJozc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters\n",
        "VOCAB_SIZE = len(sorted_words)\n",
        "EMBEDDING_DIM = 300\n",
        "MAX_LEN = np.max(df['review_len'])"
      ],
      "metadata": {
        "id": "nxE1pducJozc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_dataprep(row_data) :\n",
        "    tokenizer = Tokenizer(num_words=VOCAB_SIZE , oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(row_data)\n",
        "    seqs = tokenizer.texts_to_sequences(row_data)\n",
        "    pad_seqs = pad_sequences(seqs , maxlen = MAX_LEN , padding='post')\n",
        "\n",
        "    return pad_seqs"
      ],
      "metadata": {
        "id": "WPvqoZAhJozf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = lstm_dataprep(X_train)\n",
        "X_val = lstm_dataprep(X_val)"
      ],
      "metadata": {
        "id": "rpg6mB1YJozg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape , X_val.shape"
      ],
      "metadata": {
        "id": "PrrGXdrzJozg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(VOCAB_SIZE + 1 , EMBEDDING_DIM , input_length=MAX_LEN) ,\n",
        "    Bidirectional(LSTM(265 , return_sequences=True)) ,\n",
        "    Bidirectional(LSTM(128)) ,\n",
        "    Dense(64 , activation='relu') ,\n",
        "    Dense(1 , activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "uVJrhlO8Jozg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "I_F4GVptJozg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics=['accuracy', Precision(name = 'precision'), Recall(name = 'recall')])"
      ],
      "metadata": {
        "id": "UWO0Rd_eJozh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train ,\n",
        "    Y_train ,\n",
        "    epochs = 5 ,\n",
        "    batch_size = 64 ,\n",
        "    validation_data=(X_val , Y_val)\n",
        ")"
      ],
      "metadata": {
        "id": "lNLZxrLVJozh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc3 = history.history['accuracy'][-1]\n",
        "val_acc3 = history.history['val_accuracy'][-1]"
      ],
      "metadata": {
        "id": "2fcPtW4yJozh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The training loss for LSTM is : {history.history['loss'][-1]:0.2f}\\n\")\n",
        "print(f\"The training accuracy for LSTM is : {(history.history['accuracy'][-1]*100):0.2f}%\\n\")\n",
        "print(f\"The training precision for LSTM is : {history.history['precision'][-1]:0.2f}\\n\")\n",
        "print(f\"The training recall for LSTM is : {history.history['recall'][-1]:0.2f}\\n\")"
      ],
      "metadata": {
        "id": "N3UAiwoDJozi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The validation loss for LSTM is : {history.history['val_loss'][-1]:0.2f}\\n\")\n",
        "print(f\"The validation accuracy for LSTM is : {(history.history['val_accuracy'][-1]*100):0.2f}%\\n\")\n",
        "print(f\"The validation precision for LSTM is : {history.history['val_precision'][-1]:0.2f}\\n\")\n",
        "print(f\"The validation recall of for LSTM is : {history.history['val_recall'][-1]:0.2f}\\n\")"
      ],
      "metadata": {
        "id": "iUNL9T_1Jozi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplots(figsize=(6,4))\n",
        "plt.plot(history.history['loss'] , label='training')\n",
        "plt.plot(history.history['val_loss'] , label='validation')\n",
        "\n",
        "plt.title('Training/Validation loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(figsize=(6,4))\n",
        "plt.plot(history.history['accuracy'], label='training')\n",
        "\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "\n",
        "plt.title('Training/Validation accuracy over Epochs')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(figsize=(6,4))\n",
        "plt.plot(history.history['precision'], label='training')\n",
        "\n",
        "plt.plot(history.history['val_precision'], label='validation')\n",
        "\n",
        "plt.title('Training/Validation precision over Epochs')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(figsize=(6,4))\n",
        "plt.plot(history.history['recall'], label='training')\n",
        "\n",
        "plt.plot(history.history['val_recall'], label='validation')\n",
        "\n",
        "plt.title('Training/Validation recall over Epochs')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bx-axqWmJozi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred = model.predict(X_val)\n",
        "y_val_pred = y_val_pred.round()"
      ],
      "metadata": {
        "id": "CHqdq_RrJozj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_cm = confusion_matrix(Y_val , y_val_pred)\n",
        "sns.heatmap(lstm_cm, annot=True,fmt='3g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VG6AKMgmJozj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare between models"
      ],
      "metadata": {
        "id": "zkGdJocSJozj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_scores=[train_acc1,train_acc2,train_acc3]\n",
        "val_scores=[val_acc1,val_acc2,val_acc3]\n",
        "\n",
        "models = ['Logistic Regression','RandomForest','LSTM']\n",
        "\n",
        "x = np.arange(len(models))\n",
        "\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "rects1 = ax.bar(x - width, train_scores, width, label='Train Accuracy')\n",
        "\n",
        "rects2 = ax.bar(x + width, val_scores, width, label='Validation Accuracy')\n",
        "\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Comparison of Training and Validation Accuracies')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{:.3f}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 2),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aJqVues2Jozj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
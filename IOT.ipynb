{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Random Forest Environmental Sensor Telemetry",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sifatkhan-1915020/deeplearning-/blob/main/IOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'environmental-sensor-data-132k:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F788816%2F1355729%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241012%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241012T152600Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D80c14499ba64c4d7e4d17e7ec275c2a8aff59a4d22e16fcd3aad0160d9faf50abf9c1ad28100e2410e07cec58abf31e5d98ba98f4349761607037e65a73b057eb71745e4747220e19ccff576e084f28a53e58f4b5526b87217c7bf8f90a72cc774486e36e213b8e860a0b500ce24cd9db3e98d88d9dd28f023c0c90055fc642a73339ecfacd71d08f284d0adc6e4541ab01b6546bcbe587a641e1fb65f9de47419ff3f21099a427c59ab541ea8b1bcfa5b08cf499a4ae0e0a353152274aedf49064ebfab4c7abe36332f72ef916eb9de0a8b6e775002f918424d105859469722d3776202cf2203cd6b81b4261a01bd45389cf65232960b670dc6c92267247b0c'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "PBAOKvt6Vjfn"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ACtQtUV6Vjfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as sm\n",
        "from sklearn import metrics\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "trusted": true,
        "id": "wJSl0TioVjfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREPROCESSING**"
      ],
      "metadata": {
        "id": "spg94otJVjfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('../input/environmental-sensor-data-132k/iot_telemetry_data.csv')\n",
        "print(data.isnull().sum())\n",
        "devices = data.device.unique()\n",
        "data = data.replace(devices, ['device 1', 'device 2', 'device 3'])\n",
        "data['time'] = pd.to_datetime(data['ts'], unit='s')\n",
        "data.drop('ts', inplace=True, axis=1)\n",
        "data['light'] = data['light'].astype('int')\n",
        "data['motion'] = data['motion'].astype('int')\n",
        "print(data.corr())"
      ],
      "metadata": {
        "trusted": true,
        "id": "yoJBSCyGVjfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_1 = data[data.device == 'device 1']\n",
        "dev_2 = data[data.device == 'device 2']\n",
        "dev_3 = data[data.device == 'device 3']"
      ],
      "metadata": {
        "trusted": true,
        "id": "Fp6FeczyVjfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FUNCTIONS"
      ],
      "metadata": {
        "id": "-7yWu7YlVjfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FindLocalMin(numbers):\n",
        "    minima = []\n",
        "    length = len(numbers)\n",
        "    for n in range(1, length - 1):\n",
        "        if numbers[n] <= numbers[n - 1] and numbers[n] <= numbers[n + 1]:\n",
        "            minima.append(numbers[n])\n",
        "        if numbers[length - 1] <= numbers[length - 2]:\n",
        "            minima.append(numbers[length - 1])\n",
        "    return min(minima)\n",
        "\n",
        "def data_clean(x, y, x_label, y_label, graphs):\n",
        "    MSE = []\n",
        "    rn = range(1000, 3100, 100)\n",
        "    for s in rn:\n",
        "        EMA = y.ewm(span=s, adjust=False).mean()\n",
        "        weight = abs(1 / (.1 + y - EMA))\n",
        "        # 0.1 is a small constant offset used to avoid division by zero\n",
        "        mse = round(sm.mean_squared_error(y, EMA, sample_weight=weight), 2)\n",
        "        MSE.append(mse)\n",
        "\n",
        "    loc_min = FindLocalMin(MSE)\n",
        "    idx_first_loc_min = MSE.index(loc_min)\n",
        "    best_ema = y.ewm(span=rn[idx_first_loc_min], adjust=False).mean()\n",
        "\n",
        "    if graphs == True:\n",
        "        plt.figure(figsize=(25, 10))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(x, y, color='lime', label='Data')\n",
        "        plt.plot(x, best_ema, color='blue', linewidth=2, label='EMA')\n",
        "        plt.xlabel(x_label, fontsize=18)\n",
        "        plt.ylabel(y_label, fontsize=18)\n",
        "        plt.legend(loc='upper right', prop={'size': 20})\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(rn, MSE, label='MSE')\n",
        "        plt.plot(rn[idx_first_loc_min], loc_min, marker='o', color='red', label='ideal n of data')\n",
        "        plt.xlabel('number of data', fontsize=18)\n",
        "        plt.ylabel('MSE', fontsize=18)\n",
        "        plt.legend(loc='upper left', prop={'size': 20})\n",
        "\n",
        "    plt.subplots_adjust(wspace=.1)\n",
        "    plt.show()\n",
        "\n",
        "    return best_ema\n",
        "\n",
        "def conf_mat(test, pred):\n",
        "    print('Accuracy=', round(metrics.accuracy_score(test, pred), 2))\n",
        "    c_m = metrics.confusion_matrix(test, pred)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.heatmap(c_m/np.sum(c_m), annot=True, cmap='Blues_r', fmt='.2%',\n",
        "                xticklabels=['absence', 'presence'],\n",
        "                yticklabels=['absence', 'presence'])\n",
        "    plt.xlabel('Predicted labels', fontsize=18)\n",
        "    plt.ylabel('True labels', fontsize=18)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "0UgXTQ1wVjfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data_clean function can be divided into three steps:\n",
        "1. calculates EMA (exponential moving average) considering different amounts of data; it has the characteristic of taking into account more the recent values than previous ones. Furthermore, the original data contains outliers that differ significantly from the others, so a weight is considered in such a way that the farther the value is from the moving average, the less it is considered (0.1 is a small constant offset used to avoid division by zero);\n",
        "\n",
        "2. finds the ideal amount of EMA data via the MSE method;\n",
        "\n",
        "3.  constructs two adjacent graphs representing the original data superimposed on EMA (left) and the MSE values for the different amounts of data and the value in red for the ideal one (right)."
      ],
      "metadata": {
        "id": "tT9Pv8E4Vjfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_1.corr())"
      ],
      "metadata": {
        "trusted": true,
        "id": "zu2ihbz-Vjfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I apply the data_clean function on the 'humidity', 'smoke' and 'temperature' parameters no taking into account the 'co' and 'gpl' parameters as, together with the 'smoke' parameter, they have a covariance value close to 1 as proof which are highly correlated."
      ],
      "metadata": {
        "id": "Ut3Vqk0_Vjfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dc_1_co = data_clean(dev_1.time, dev_1.co, x_label='time', y_label='co [ppm]', graphs=True)\n",
        "dc_1_hum = data_clean(dev_1.time, dev_1.humidity, x_label='time', y_label='humidity [%]', graphs=True)\n",
        "# dc_1_lpg = data_clean(dev_1.time, dev_1.lpg, x_label='time', y_label='LPG [ppm]', graphs=True)\n",
        "dc_1_smoke = data_clean(dev_1.time, dev_1.smoke, x_label='time', y_label='smoke [ppm]', graphs=True)\n",
        "dc_1_temp = data_clean(dev_1.time, dev_1.temp, x_label='time', y_label='temp [°F]', graphs=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "20bkYKA7Vjfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'time': dev_1.time,\n",
        "                   # 'co': dc_1_co,\n",
        "                   'humidity': dc_1_hum,\n",
        "                   # 'lpg': dc_1_lpg,\n",
        "                   'smoke': dc_1_smoke,\n",
        "                   'temp': dc_1_temp,\n",
        "                   'light': dev_1.light,\n",
        "                   'motion': dev_1.motion})\n",
        "\n",
        "df['pres'] = np.zeros(len(df), dtype=int)\n",
        "df = df.sample(frac=.05, random_state=154)\n",
        "df = df.sort_values(by='time', ascending=True)\n",
        "df.index = range(0, len(df))"
      ],
      "metadata": {
        "trusted": true,
        "id": "eh-oMdb-Vjfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I consider an increase in parameters and the activation of the light and motion sensor as the fact that there is at least one person near the device."
      ],
      "metadata": {
        "id": "E1rukDvfVjfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_col = df.columns[1:]\n",
        "\n",
        "for i in name_col[:-1]:\n",
        "    if df[i].dtype == 'float':\n",
        "        for j in df.index:\n",
        "            if j == 0 or df.pres[j] == 1:\n",
        "                pass\n",
        "            elif df[i][j - 1] < df[i][j] and df[i][j] - df[i][j - 1] > .0008:\n",
        "                # .0008 is a random number but it must be replaced with other values such as, for example, the error of each sensor (error device[i]), in order not to consider an increase in the value below a certain threshold.\n",
        "                df.loc[j, 'pres'] = 1\n",
        "    else:\n",
        "        for j in df.index:\n",
        "            if df.pres[j] == 1:\n",
        "                pass\n",
        "            elif df[i][j] == 1:\n",
        "                df.loc[j, 'pres'] = 1\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "hXRASHElVjfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RANDOM FOREST"
      ],
      "metadata": {
        "id": "UiJzC77jVjfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = df[name_col[:-1]], df[name_col[-1]]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=123)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "8ZejzUx5Vjfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I look for the optimal number of estimators that allows me to lower the OOB errors as much as possible."
      ],
      "metadata": {
        "id": "W5UbC4zYVjfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oob_error = []\n",
        "rn_est = list(range(30, 205, 5))\n",
        "for i in rn_est:\n",
        "    clf = RandomForestClassifier(max_features='sqrt', oob_score=True, n_estimators=i,\n",
        "                                 random_state=123)\n",
        "    clf.fit(X_train, y_train)\n",
        "    oob_err = 1 - clf.oob_score_\n",
        "    oob_error.append(oob_err)\n",
        "\n",
        "best_n_est = rn_est[oob_error.index(min(oob_error))]"
      ],
      "metadata": {
        "trusted": true,
        "id": "Sdx2mtU5Vjfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(rn_est, oob_error, label='OOB error rate')\n",
        "plt.plot(best_n_est, min(oob_error), marker='o', color='red', label='ideal n estimator')\n",
        "plt.xlabel('n estimators', fontsize=18)\n",
        "plt.ylabel('OOB error rate', fontsize=18)\n",
        "plt.legend(loc='upper right', prop={'size': 10})\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "aMsvPPDIVjfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF_classifier = RandomForestClassifier(max_features='sqrt', oob_score=True,\n",
        "                                       n_estimators=best_n_est, random_state=123)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "VBzs4Y4RVjfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF_classifier.fit(X_train, y_train)\n",
        "y_RF_pred = RF_classifier.predict(X_test)\n",
        "conf_mat(y_test, y_RF_pred)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_bsSGNIPVjfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance"
      ],
      "metadata": {
        "id": "ZuIkQgiEVjfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = RF_classifier.feature_importances_\n",
        "feature_names = name_col[:-1]\n",
        "feature_importance = 100 * (feature_importance / max(feature_importance))\n",
        "\n",
        "index_sorted = np.flipud(np.argsort(feature_importance))\n",
        "pos = np.arange(index_sorted.shape[0]) + 0.5\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(pos, feature_importance[index_sorted], align='center')\n",
        "plt.xticks(pos, feature_names[index_sorted], fontsize=18)\n",
        "plt.ylabel('Relative importance', fontsize=18)\n",
        "plt.title('Feature Importance', fontsize=23)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "e_3cqlw5Vjfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APPLICATION OF THE CLASSIFIER ON THE SECOND DATASET"
      ],
      "metadata": {
        "id": "-rstlf35Vjfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dc_2_co = data_clean(dev_2.time, dev_2.co, x_label='time', y_label='co [ppm]', graphs=False)\n",
        "dc_2_hum = data_clean(dev_2.time, dev_2.humidity, x_label='time', y_label='humidity [%]', graphs=False)\n",
        "# dc_2_lpg = data_clean(dev_2.time, dev_2.lpg, x_label='time', y_label='LPG [ppm]', graphs=False)\n",
        "dc_2_smoke = data_clean(dev_2.time, dev_2.smoke, x_label='time', y_label='smoke [ppm]', graphs=False)\n",
        "dc_2_temp = data_clean(dev_2.time, dev_2.temp, x_label='time', y_label='temp [°F]', graphs=False)\n",
        "\n",
        "df_2 = pd.DataFrame({'time': dev_2.time,\n",
        "                     # 'co': dc_2_co,\n",
        "                     'humidity': dc_2_hum,\n",
        "                     # 'lpg': dc_2_lpg,\n",
        "                     'smoke': dc_2_smoke,\n",
        "                     'temp': dc_2_temp,\n",
        "                     'light': dev_2.light,\n",
        "                     'motion': dev_2.motion})\n",
        "\n",
        "df_2['pres'] = np.zeros(len(df_2), dtype=int)\n",
        "df_2 = df_2.sample(frac=0.5, random_state=154)\n",
        "df_2 = df_2.sort_values(by='time', ascending=True)\n",
        "df_2.index = range(0, len(df_2))"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q3ASDhSQVjfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_RF_pred_2 = RF_classifier.predict(df_2[name_col[:-1]])"
      ],
      "metadata": {
        "trusted": true,
        "id": "V-xVT0QsVjfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df_2.time, y_RF_pred_2, linewidth=2)\n",
        "plt.xlabel('time', fontsize=18)\n",
        "plt.ylabel('presence', fontsize=18)\n",
        "plt.yticks([0, 1], fontsize=15)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "FBvcAG0OVjfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APPLICATION OF THE CLASSIFIER ON THE THIRD DATASET"
      ],
      "metadata": {
        "id": "wk3wBiiFVjfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dc_3_co = data_clean(dev_3.time, dev_3.co, x_label='time', y_label='co [ppm]', graphs=False)\n",
        "dc_3_hum = data_clean(dev_3.time, dev_3.humidity, x_label='time', y_label='humidity [%]', graphs=False)\n",
        "# dc_3_lpg = data_clean(dev_3.time, dev_3.lpg, x_label='time', y_label='LPG [ppm]', graphs=False)\n",
        "dc_3_smoke = data_clean(dev_3.time, dev_3.smoke, x_label='time', y_label='smoke [ppm]', graphs=False)\n",
        "dc_3_temp = data_clean(dev_3.time, dev_3.temp, x_label='time', y_label='temp [°F]', graphs=False)\n",
        "\n",
        "df_3 = pd.DataFrame({'time': dev_3.time,\n",
        "                     # 'co': dc_3_co,\n",
        "                     'humidity': dc_3_hum,\n",
        "                     # 'lpg': dc_3_lpg,\n",
        "                     'smoke': dc_3_smoke,\n",
        "                     'temp': dc_3_temp,\n",
        "                     'light': dev_3.light,\n",
        "                     'motion': dev_3.motion})\n",
        "\n",
        "df_3['pres'] = np.zeros(len(df_3), dtype=int)\n",
        "df_3 = df_3.sample(frac=0.5, random_state=154)\n",
        "df_3 = df_3.sort_values(by='time', ascending=True)\n",
        "df_3.index = range(0, len(df_3))"
      ],
      "metadata": {
        "trusted": true,
        "id": "apNwORmdVjft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_RF_pred_3 = RF_classifier.predict(df_3[name_col[:-1]])"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ug4nMVlrVjft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df_3.time, y_RF_pred_3, linewidth=2)\n",
        "plt.xlabel('time', fontsize=18)\n",
        "plt.ylabel('presence', fontsize=18)\n",
        "plt.yticks([0, 1], fontsize=15)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "wIW53lOPVjft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to implement this program would be to consider the number of increasing parameters such as, if they are more than a certain percentage of the number of parameters, the device considers that fact to be someone's presence. Each parameter must be assigned a probability such as, if the light or motion sensor is activated, I'm sure there is someone near the device, so I can assign them a P = 1. For the CO sensor a different one can be assigned because increasing that parameter may mean that the heating system is faulty. This approach can be applied for the other sensors:\n",
        "* smoke and temperature sensors: it can mean that something is taking fire;\n",
        "* LPG sensor: if the device is mounted on an LPG car, it may mean that the LPG cylinder is leaking gas."
      ],
      "metadata": {
        "id": "6wgGMo8FVjft"
      }
    }
  ]
}
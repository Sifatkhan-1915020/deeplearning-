{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2qu3i+hVPZpiZVpHtQpaX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sifatkhan-1915020/deeplearning-/blob/main/LSTM%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "80_5YjSmpT9S"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x) ** 2"
      ],
      "metadata": {
        "id": "uXUJvlWlpfAV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_lstm_parameters(input_size, hidden_size):\n",
        "    parameters = {}\n",
        "    parameters['Wf'] = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
        "    parameters['bf'] = np.zeros((hidden_size, 1))\n",
        "    parameters['Wi'] = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
        "    parameters['bi'] = np.zeros((hidden_size, 1))\n",
        "    parameters['Wo'] = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
        "    parameters['bo'] = np.zeros((hidden_size, 1))\n",
        "    parameters['Wc'] = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
        "    parameters['bc'] = np.zeros((hidden_size, 1))\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "dtsBeolcpjYl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "    Wf = parameters['Wf']\n",
        "    bf = parameters['bf']\n",
        "    Wi = parameters['Wi']\n",
        "    bi = parameters['bi']\n",
        "    Wo = parameters['Wo']\n",
        "    bo = parameters['bo']\n",
        "    Wc = parameters['Wc']\n",
        "    bc = parameters['bc']\n",
        "\n",
        "    concat = np.concatenate((a_prev, xt), axis=0)\n",
        "\n",
        "    ft = sigmoid(np.dot(Wf, concat) + bf)\n",
        "    it = sigmoid(np.dot(Wi, concat) + bi)\n",
        "    cct = tanh(np.dot(Wc, concat) + bc)\n",
        "    c_next = ft * c_prev + it * cct\n",
        "    ot = sigmoid(np.dot(Wo, concat) + bo)\n",
        "    a_next = ot * tanh(c_next)\n",
        "\n",
        "    return a_next, c_next\n"
      ],
      "metadata": {
        "id": "FQJU1FgKpxVl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_forward(x, a0, parameters):\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_a, m = a0.shape\n",
        "\n",
        "    a = np.zeros((n_a, m, T_x))\n",
        "    c = np.zeros((n_a, m, T_x))\n",
        "    a_next = a0\n",
        "    c_next = np.zeros((n_a, m))\n",
        "\n",
        "    for t in range(T_x):\n",
        "        a_next, c_next = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)\n",
        "        a[:,:,t] = a_next\n",
        "        c[:,:,t] = c_next\n",
        "\n",
        "    return a, c\n"
      ],
      "metadata": {
        "id": "OfqrubgCp_vG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(y, y_hat):\n",
        "    loss = np.sum((y - y_hat) ** 2) / y.shape[1]\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "Fh4yM0vlq5NA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_backward(da, caches, parameters):\n",
        "    (a_next, c_next, a, c, ft, it, cct, ot) = caches[-1]\n",
        "    n_a, m, T_x = da.shape\n",
        "\n",
        "    gradients = {}\n",
        "\n",
        "    gradients['dWf'] = np.zeros_like(parameters['Wf'])\n",
        "    gradients['dbf'] = np.zeros_like(parameters['bf'])\n",
        "    gradients['dWi'] = np.zeros_like(parameters['Wi'])\n",
        "    gradients['dbi'] = np.zeros_like(parameters['bi'])\n",
        "    gradients['dWo'] = np.zeros_like(parameters['Wo'])\n",
        "    gradients['dbo'] = np.zeros_like(parameters['bo'])\n",
        "    gradients['dWc'] = np.zeros_like(parameters['Wc'])\n",
        "    gradients['dbc'] = np.zeros_like(parameters['bc'])\n",
        "\n",
        "    da_prev = np.zeros_like(a_next)\n",
        "    dc_prev = np.zeros_like(c_next)\n",
        "\n",
        "    for t in reversed(range(T_x)):\n",
        "        a_next, c_next, a_prev, c_prev, ft, it, cct, ot = caches[t]\n",
        "\n",
        "        dot = da[:,:,t] * tanh(c_next) * sigmoid_derivative(ot)\n",
        "        dft = da[:,:,t] * dc_prev * c_prev * sigmoid_derivative(ft)\n",
        "        dit = da[:,:,t] * dc_prev * cct * sigmoid_derivative(it)\n",
        "        dcct = da[:,:,t] * dc_prev * it * tanh_derivative(cct)\n",
        "\n",
        "        concat = np.concatenate((a_prev, a[:,:,t]), axis=0)\n",
        "\n",
        "        gradients['dWf'] += np.dot(dft, concat.T)\n",
        "        gradients['dbf'] += np.sum(dft, axis=1, keepdims=True)\n",
        "        gradients['dWi'] += np.dot(dit, concat.T)\n",
        "        gradients['dbi'] += np.sum(dit, axis=1, keepdims=True)\n",
        "        gradients['dWo'] += np.dot(dot, concat.T)\n",
        "        gradients['dbo'] += np.sum(dot, axis=1, keepdims=True)\n",
        "        gradients['dWc'] += np.dot(dcct, concat.T)\n",
        "        gradients['dbc'] += np.sum(dcct, axis=1, keepdims=True)\n",
        "\n",
        "        da_prev = np.dot(parameters['Wf'][:, :n_a].T, dft) + np.dot(parameters['Wi'][:, :n_a].T, dit) + np.dot(parameters['Wo'][:, :n_a].T, dot) + np.dot(parameters['Wc'][:, :n_a].T, dcct)\n",
        "        dc_prev = ft * dc_prev + it * dcct * sigmoid_derivative(cct)\n",
        "\n",
        "    return gradients\n"
      ],
      "metadata": {
        "id": "vgrEaupRqIYu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    parameters['Wf'] -= learning_rate * gradients['dWf']\n",
        "    parameters['bf'] -= learning_rate * gradients['dbf']\n",
        "    parameters['Wi'] -= learning_rate * gradients['dWi']\n",
        "    parameters['bi'] -= learning_rate * gradients['dbi']\n",
        "    parameters['Wo'] -= learning_rate * gradients['dWo']\n",
        "    parameters['bo'] -= learning_rate * gradients['dbo']\n",
        "    parameters['Wc'] -= learning_rate * gradients['dWc']\n",
        "    parameters['bc'] -= learning_rate * gradients['dbc']\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "tqfazloKqnZP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm(x_train, y_train, parameters, epochs, learning_rate):\n",
        "    n_x, m, T_x = x_train.shape\n",
        "    n_y, m, T_y = y_train.shape\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        a0 = np.zeros((parameters['Wf'].shape[0], m))\n",
        "        a, c, caches = lstm_forward(x_train, a0, parameters)\n",
        "        y_hat = a[:n_y,:,:]\n",
        "\n",
        "        loss = compute_loss(y_train, y_hat)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        da = y_hat - y_train\n",
        "\n",
        "        gradients = lstm_backward(da, caches, parameters)\n",
        "\n",
        "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    return parameters, loss_history\n"
      ],
      "metadata": {
        "id": "r5uZXPVMq92o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lstm(x_test, parameters):\n",
        "    a0 = np.zeros((parameters['Wf'].shape[0], x_test.shape[1]))\n",
        "    a, c, _ = lstm_forward(x_test, a0, parameters)\n",
        "    return a[:x_test.shape[0],:,:]\n"
      ],
      "metadata": {
        "id": "TqwqWwqcq_zh"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}